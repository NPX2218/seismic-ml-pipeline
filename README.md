<h1 align="center">
Seismic ML Data Pipeline
</h1>

<p align="center">
A signal processing pipeline for extracting machine learning features from seismic waveform data
</p>

<p align="center">
  <img src="https://img.shields.io/badge/python-3.10+-blue.svg" alt="Python 3.10+">
  <img src="https://img.shields.io/badge/license-MIT-green.svg" alt="MIT License">
</p>

## Description

This pipeline processes seismic time series data from the Alaska Earthquake Center and extracts features for AI/ML-based earthquake prediction. Rather than using traditional spectrograms, this approach converts waveform data into histograms of amplitude distributions within specific frequency bands, capturing the statistical properties of seismic signals.

## Pipeline Architecture

```
Raw miniSEED --> Load & Validate --> Sliding Windows --> Bandpass Filter --> Feature Extraction --> Feature Matrix
```

## How It Works (Step-by-Step)

### Step 1: Data Loading

The pipeline reads miniSEED files, which are the standard format for seismic waveform data. Each file contains:

- **Waveform data**: The actual ground motion measurements (amplitude over time)
- **Metadata**: Station name, network code, sampling rate, start/end times
- **Multiple channels**: Typically 3 channels measuring motion in different directions (vertical, north-south, east-west)

```python
# ObsPy handles miniSEED parsing
from obspy import read
st = read("data/raw/station.mseed")  # Returns a Stream object with Traces
```

### Step 2: Sliding Windows

The continuous seismic data is split into overlapping time windows. This is because:

- Earthquake precursors may only appear in certain time periods
- ML models need fixed-size inputs
- Overlapping windows ensure we don't miss patterns at window boundaries

**Example with default settings:**

```
Recording: 24 hours of data
Window size (TD): 2 hours
Step size (DELTA_T): 0.5 hours

Windows generated:
  Window 1: hours 0.0 - 2.0
  Window 2: hours 0.5 - 2.5
  Window 3: hours 1.0 - 3.0
  ... and so on

Total windows = (24 - 2) / 0.5 + 1 = 45 windows
```

### Step 3: Frequency Band Decomposition

Each window is filtered into three frequency bands using Butterworth bandpass filters:

| Band | Frequency Range | What It Captures                                                      |
| ---- | --------------- | --------------------------------------------------------------------- |
| HF1  | 5 - 25 Hz       | High-frequency vibrations: local noise, small tremors, human activity |
| HF2  | 0.1 - 5 Hz      | Body waves: P-waves and S-waves from earthquakes                      |
| LF   | Residual        | Slow drift: tectonic movement, long-period surface waves              |

**Why separate frequency bands?**

Different seismic phenomena appear at different frequencies. By separating them, the ML model can learn which frequency patterns precede earthquakes.

### Step 4: Histogram-Based Feature Extraction

For each frequency band, instead of using the raw waveform, we compute a histogram of amplitude values:

```
Raw signal: [0.1, -0.3, 0.5, 0.2, -0.1, 0.8, ...]
                            |
                            v
Histogram: How many samples fall into each amplitude range?
           [-1 to -0.5]: 12 samples
           [-0.5 to 0]:  45 samples
           [0 to 0.5]:   38 samples
           [0.5 to 1]:   15 samples
```

From each histogram, we extract two features:

- **Spectral Centroid**: The "center of mass" of the distribution. Higher values mean the signal has more high-amplitude content.
- **Spectral Spread**: How spread out the amplitudes are. Higher values mean more variable signal intensity.

**Why histograms instead of spectrograms?**

Histograms compress the information into a smaller, more interpretable representation while preserving the statistical properties that may indicate earthquake precursors.

### Step 5: Feature Matrix Output

For a 3-channel recording:

```
3 channels x 3 frequency bands x 2 features = 18 features per window
```

The output is a matrix of shape `(n_windows, 18)` saved as a NumPy file.

### Step 6: Label Generation

Labels are generated by checking if an earthquake occurred after each window. This is where the external API calls come in.

## External API Calls

The pipeline makes two types of API calls to fetch earthquake-related data:

### API 1: IRIS Station Service

**Purpose**: Get the geographic coordinates (latitude/longitude) of the seismic station.

**Why needed**: To search for earthquakes near the station, we need to know where the station is located.

**Endpoint**: `https://service.iris.edu/fdsnws/station/1/query`

**Request parameters**:

```
network: Network code (e.g., "AK" for Alaska)
station: Station code (e.g., "MDM")
format: "text" (returns pipe-delimited data)
level: "station" (we only need station-level info, not channel details)
```

**Example request**:

```
https://service.iris.edu/fdsnws/station/1/query?network=AK&station=MDM&format=text&level=station
```

**Example response**:

```
#Network|Station|Latitude|Longitude|Elevation|...
AK|MDM|63.8831|-149.1697|411.0|...
```

**Caching**: Station coordinates are cached locally in `data/cache/station_coords.json` to avoid repeated API calls for the same station.

### API 2: USGS Earthquake Catalog

**Purpose**: Get a list of earthquakes that occurred near the station during and after the recording.

**Why needed**: To create labels for supervised learning. We label a window as "positive" if an earthquake occurs within the lookahead period after that window.

**Endpoint**: `https://earthquake.usgs.gov/fdsnws/event/1/query`

**Request parameters**:

```
format: "csv" (returns comma-separated data)
starttime: Recording start minus buffer (e.g., 48 hours before)
endtime: Recording end plus buffer (e.g., 48 hours after)
latitude: Station latitude (from IRIS API)
longitude: Station longitude (from IRIS API)
maxradiuskm: Search radius (default 500 km)
minmagnitude: Minimum magnitude to include (default 2.0)
orderby: "time" (chronological order)
```

**Example request**:

```
https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&starttime=2023-10-13&endtime=2023-10-17&latitude=63.88&longitude=-149.17&maxradiuskm=500&minmagnitude=2.0
```

**Example response**:

```csv
time,latitude,longitude,depth,mag,place
2023-10-15T14:32:00.000Z,63.12,-150.45,10.2,3.2,"45km N of Talkeetna, Alaska"
2023-10-15T18:45:00.000Z,64.01,-148.90,8.5,2.8,"30km E of Fairbanks, Alaska"
```

**Buffer time**: We query 48 hours before and after the recording because:

- Before: Provides context about recent seismic activity
- After: Essential for labeling. A window at hour 23 of a 24-hour recording needs to know about earthquakes at hour 29 (6 hours later) to be correctly labeled.

**Caching**: Earthquake catalogs are cached locally in `data/catalogs/` to avoid repeated API calls.

### Label Assignment Logic

For each time window, we check: "Does an earthquake occur within LOOKAHEAD_HOURS after this window ends?"

```
Window ends at hour 10.0
Lookahead: 6 hours
Check period: hours 10.0 to 16.0

If any earthquake with magnitude >= MIN_MAGNITUDE occurs during hours 10-16:
    Label = 1 (positive - earthquake coming)
Else:
    Label = 0 (negative - no earthquake)
```

## Project Structure

```
seismic-ml-pipeline/
├── main.py                     # Main entry point
├── visualize.py                # Generate diagnostic plots
├── requirements.txt            # Dependencies
├── data/
│   ├── raw/                    # Input miniSEED files
│   ├── processed/              # Output features and labels
│   ├── catalogs/               # Earthquake catalogs from USGS (cached)
│   └── cache/                  # Station coordinates (cached)
└── src/
    ├── loader/
    │   └── loader.py           # miniSEED file loading
    ├── preprocessing/
    │   ├── windows.py          # Sliding window generation
    │   ├── features.py         # Feature extraction
    │   ├── labels.py           # Label generation
    │   └── catalog.py          # USGS catalog fetching
    └── helpers/
        └── helper.py           # Utility functions
```

## Installation

```bash
git clone https://github.com/NPX2218/seismic-ml-pipeline.git
cd seismic-ml-pipeline
pip install -r requirements.txt
```

### Dependencies

- Python 3.10+
- NumPy
- ObsPy
- SciPy
- Pandas
- Requests
- Matplotlib (for visualization)

## Usage

### Process all files in the default directory

```bash
python main.py
```

This processes all `.mseed` files in `data/raw/`.

### Process a single file

```bash
python main.py data/raw/station_data.mseed
```

### Process files in a specific directory

```bash
python main.py /path/to/mseed/folder
```

### Generate visualizations

After processing, generate diagnostic plots:

```bash
python visualize.py data/processed/STATION_DATE/
```

## Configuration

Pipeline parameters can be modified at the top of `main.py`:

| Parameter         | Default | Description                                      |
| ----------------- | ------- | ------------------------------------------------ |
| `TD`              | 2.0     | Window duration in hours                         |
| `DELTA_T`         | 0.5     | Step size between windows in hours               |
| `N_BINS`          | 100     | Number of histogram bins                         |
| `LOOKAHEAD_HOURS` | 6.0     | Hours ahead to check for earthquakes             |
| `MIN_MAGNITUDE`   | 3.0     | Minimum earthquake magnitude for positive labels |
| `CHANNELS`        | None    | Channel indices to use (None = all)              |

## Output Files

For each processed file, the pipeline creates a folder in `data/processed/{station}_{date}/` containing:

| File                    | Description                                        |
| ----------------------- | -------------------------------------------------- |
| `features.npy`          | Feature matrix (n_windows x n_features)            |
| `labels.npy`            | Binary labels for each window                      |
| `timestamps.npy`        | Start and end times for each window                |
| `feature_names.txt`     | Names of each feature column                       |
| `config.json`           | Pipeline configuration used                        |
| `label_config.json`     | Labeling parameters                                |
| `catalog_processed.csv` | Earthquake catalog with hours from recording start |

## Visualization Outputs

Running `visualize.py` generates:

| File                            | Description                                  |
| ------------------------------- | -------------------------------------------- |
| `viz_feature_distributions.png` | Histogram of each feature                    |
| `viz_feature_timeseries.png`    | Features over time with earthquake markers   |
| `viz_label_timeline.png`        | Label assignments and earthquake occurrences |
| `viz_features_by_label.png`     | Feature distributions split by label         |
| `viz_correlation_matrix.png`    | Correlation between features                 |
| `viz_summary_dashboard.png`     | Overview dashboard with key statistics       |

## Example Workflow

```bash
# 1. Download miniSEED data from IRIS or Alaska Earthquake Center
#    Place files in data/raw/

# 2. Process all files
python main.py

# 3. Check the output
ls data/processed/

# 4. Generate visualizations for a specific station
python visualize.py data/processed/MDM_20231015/

# 5. Use features.npy and labels.npy for ML training
```

## Authors

- **Neel Bansal** - [LinkedIn](https://www.linkedin.com/in/neel-bansal/)
- **Anders Haroldson** - [LinkedIn](https://www.linkedin.com/in/anders-haroldson/)

## Acknowledgments

- [Alaska Earthquake Center](https://earthquake.alaska.edu/) for seismic data
- [USGS Earthquake Hazards Program](https://earthquake.usgs.gov/) for earthquake catalogs
- [NSF SAGE / IRIS](https://www.iris.edu/hq/) for data infrastructure
- [ObsPy](https://obspy.org/) for seismic data processing
- [NumPy](https://numpy.org/) and [SciPy](https://scipy.org/) for numerical computing
- [Pandas](https://pandas.pydata.org/) for data manipulation

## License

This project is under a Restrictive License. The code is publicly visible for educational purposes only. Any use, including commercial, research, or personal projects, requires explicit written permission from the authors. See [LICENSE](LICENSE) for details.
